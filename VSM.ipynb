{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10364875",
   "metadata": {},
   "source": [
    "Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "24d8ef5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import chardet\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "from tkinter import scrolledtext\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0117f5ca",
   "metadata": {},
   "source": [
    "Function to read documents from a directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "481f7e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_documents(directory):\n",
    "    universal_set = set()\n",
    "    documents = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.txt'):\n",
    "            doc_id = os.path.splitext(file_name)[0]\n",
    "            with open(os.path.join(directory, file_name), 'r', encoding='ISO-8859-1') as file:\n",
    "                content = file.read()\n",
    "                documents[doc_id] = content\n",
    "                universal_set.add(doc_id)\n",
    "    return documents, universal_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe7e3cb",
   "metadata": {},
   "source": [
    "Function to preprocess the content (lowercasing, tokenization, stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "603dede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(content):\n",
    "    tokens = re.findall(r'\\b\\w+\\b', content.lower())\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [porter_stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db063b03",
   "metadata": {},
   "source": [
    "Function to build index and calculate tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a5028c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index_docs(documents):\n",
    "    index = {}\n",
    "    for doc_id, doc_content in documents.items():\n",
    "        terms = preprocess(doc_content)\n",
    "        for term in terms:\n",
    "            if term not in index:\n",
    "                index[term] = {}\n",
    "            if doc_id not in index[term]:\n",
    "                index[term][doc_id] = 0\n",
    "            index[term][doc_id] += 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d273c125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index_q(query):\n",
    "    index = {}\n",
    "    terms = preprocess(query)\n",
    "    for term in terms:\n",
    "        if term not in index:\n",
    "            index[term] = {}\n",
    "            index[term] = 0\n",
    "        index[term] += 1\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72978a7",
   "metadata": {},
   "source": [
    "Function to compute IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "79a9ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(token, index, total_docs):\n",
    "    if token in index.keys():\n",
    "        df = len(index[token])\n",
    "        if df == 0:\n",
    "            return 0\n",
    "        idf = math.log(total_docs / df)\n",
    "        return idf\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d5cb4",
   "metadata": {},
   "source": [
    "Function to build document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40d15766",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_document_vectors(documents, index, total_docs):\n",
    "    document_vectors = {}\n",
    "    for doc_id, doc_text in documents.items():\n",
    "        vector = {}\n",
    "        for token in index.keys():\n",
    "            if doc_id in index[token]:\n",
    "                tf = index[token][doc_id]\n",
    "            else:\n",
    "                tf = 0\n",
    "            idf = compute_idf(token, index, total_docs)\n",
    "            vector[token] = tf*idf\n",
    "        document_vectors[doc_id] = vector\n",
    "    return document_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca743d",
   "metadata": {},
   "source": [
    "Function to build query vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5ce68f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_query_vectors(index, query, q_index, total_docs):\n",
    "    vector = {}\n",
    "    for token in index.keys():\n",
    "        if token in q_index.keys():\n",
    "            tf = q_index[token]\n",
    "            idf = compute_idf(token, index, total_docs)\n",
    "            vector[token] = tf*idf\n",
    "        else:\n",
    "            vector[token] = 0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9f8f49",
   "metadata": {},
   "source": [
    "Function to compute cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c481ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(query_vector, doc_vector):\n",
    "    \n",
    "    if isinstance(doc_vector, dict):\n",
    "        doc_vector = np.array(list(doc_vector.values()))\n",
    "    if isinstance(query_vector, dict):\n",
    "        query_vector = np.array(list(query_vector.values()))\n",
    "    \n",
    "    return np.dot(doc_vector, query_vector) / (np.linalg.norm(doc_vector) * (np.linalg.norm(query_vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51bd1ed9",
   "metadata": {},
   "source": [
    "Function to process queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0e6f9123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_query(index, docs_vectors, query, universal_set):\n",
    "    \n",
    "    q_index = build_index_q(query)\n",
    "    query_vector = build_query_vectors(index, query, q_index, len(universal_set))\n",
    "\n",
    "    sim = {}\n",
    "    for doc in docs_vectors.keys():\n",
    "        sim[int(doc)] = compute_cosine_similarity(query_vector, docs_vectors[doc])\n",
    "    \n",
    "    sorted_docs = sorted(sim.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = []\n",
    "    for i in range(len(sorted_docs)):\n",
    "        if sorted_docs[i][1] >= 0.05:\n",
    "            result.append(sorted_docs[i][0])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a4f5fc",
   "metadata": {},
   "source": [
    "Function to save document vectors and universal set to pickle files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8f063731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_indexes(document_vectors, document_index, universal_set):\n",
    "    with open('document_vectors.pkl', 'wb') as file:\n",
    "        pickle.dump(document_vectors, file)\n",
    "        \n",
    "    with open('document_index.pkl', 'wb') as file:\n",
    "        pickle.dump(document_index, file)\n",
    "        \n",
    "    with open('universal_set.pkl', 'wb') as file:\n",
    "        pickle.dump(universal_set, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78ddd6",
   "metadata": {},
   "source": [
    "Function to load indexes and universal set from pickle files if they exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a4531e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indexes():\n",
    "    document_vectors_file = 'document_vectors.pkl'\n",
    "    universal_set_file = 'universal_set.pkl'\n",
    "    document_index_file = 'document_index.pkl'\n",
    "\n",
    "    if os.path.exists(document_vectors_file) and os.path.exists(universal_set_file):\n",
    "        with open(document_vectors_file, 'rb') as file:\n",
    "            document_vectors = pickle.load(file)\n",
    "            \n",
    "        with open(document_index_file, 'rb') as file:\n",
    "            document_index = pickle.load(file)\n",
    "\n",
    "        with open(universal_set_file, 'rb') as file:\n",
    "            universal_set = pickle.load(file)\n",
    "        \n",
    "        return document_vectors, document_index, universal_set\n",
    "    else:\n",
    "        universal_set = set()\n",
    "        files_directory = 'C:/Users/UBL-HO.DESKTOP-7ET3E40/Desktop/BAI-6A/IR/a/ResearchPapers'\n",
    "    \n",
    "        documents, universal_set = read_documents(files_directory)\n",
    "        \n",
    "        index = build_index_docs(documents)\n",
    "        docs_vectors = build_document_vectors(documents, index, len(universal_set))\n",
    "        \n",
    "        save_indexes(docs_vectors, index, universal_set)\n",
    "\n",
    "        return docs_vectors, index, universal_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6ad518",
   "metadata": {},
   "source": [
    "Function to retrieve results based on the entered query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "64ce7078",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_results():\n",
    "    query = query_entry.get()\n",
    "    result_text.config(state='normal')\n",
    "    result_text.delete('1.0', tk.END)\n",
    "    \n",
    "    result = process_query(index, docs_vectors, query, universal_set)\n",
    "    \n",
    "    result_text.insert(tk.END, f\"\\nRetrieved Documents: {result}\\n\", 'result')\n",
    "    \n",
    "    result_text.config(state='disabled')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2df0015",
   "metadata": {},
   "source": [
    "Main part of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bb750928",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_vectors, index, universal_set = load_indexes()\n",
    "\n",
    "window = tk.Tk()\n",
    "window.title(\"Information Retrieval System\")\n",
    "window.geometry(\"800x600\")\n",
    "window.configure(background='aliceblue')\n",
    "\n",
    "query_label = tk.Label(window, text=\"Enter your query:\", font=(\"Helvetica\", 14), bg='aliceblue')\n",
    "query_label.pack(pady=10)\n",
    "\n",
    "query_entry = tk.Entry(window, width=50, font=(\"Helvetica\", 12), bg='gray95')\n",
    "query_entry.pack(pady=10)\n",
    "\n",
    "search_button = tk.Button(window, text=\"Search\", command=retrieve_results, font=(\"Helvetica\", 12), bg='gray90')\n",
    "search_button.pack(pady=10)\n",
    "\n",
    "result_text = scrolledtext.ScrolledText(window, width=100, height=10, state='disabled', font=(\"Helvetica\", 12), bg='gray95')\n",
    "result_text.pack(pady=10)\n",
    "\n",
    "result_text.tag_configure('result', foreground='green')\n",
    "\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
